{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e181e1e",
   "metadata": {},
   "source": [
    "Yes, it is possible to implement an exponential smoothing recurrent neural network (RNN) using PyTorch. Exponential smoothing is a technique that is commonly used in time series forecasting to smooth out short-term fluctuations and make it easier to predict future values.\n",
    "\n",
    "To implement an exponential smoothing RNN in PyTorch, you can use the torch.nn.Linear and torch.nn.RNN modules, which provide implementations of the linear and recurrent layers, respectively. You can also use the torch.nn.functional.exponential_smooth function to apply exponential smoothing to the output of the RNN. Here is an example of how you might use these modules and functions to define an exponential smoothing RNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d26790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SMAPELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Y_true, Y_pred):\n",
    "        # Compute the absolute difference between Y_true and Y_pred\n",
    "        diff = torch.abs(Y_true - Y_pred)\n",
    "\n",
    "        # Compute the absolute values of Y_true and Y_pred\n",
    "        true = torch.abs(Y_true)\n",
    "        pred = torch.abs(Y_pred)\n",
    "\n",
    "        # Compute the sum of the absolute differences divided by the sum of the absolute values\n",
    "        smape = torch.sum(diff / (true + pred))\n",
    "\n",
    "        # Multiply by 100 and divide by the number of samples to get the final loss\n",
    "        return 100 * smape / Y_true.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a09695f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# df=pd.read_csv('run-of-river_production_load.csv',index_col='Date_Time',parse_dates=True)\n",
    "df=pd.read_csv('run-of-river_production_load.csv',parse_dates=True)\n",
    "df[\"Date_Time\"] = pd.to_datetime(df[\"Date_Time\"])\n",
    "df[\"date\"]=df[\"Date_Time\"].dt.date\n",
    "df[\"month\"]=df[\"Date_Time\"].dt.month\n",
    "df[\"day\"]=df[\"Date_Time\"].dt.day\n",
    "df[\"hour\"]=df[\"Date_Time\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1e34ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17890ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=pd.DataFrame()\n",
    "target=pd.DataFrame()\n",
    "input['month']=df['month']\n",
    "input['day']=df['day']\n",
    "input['hour']=df['hour']\n",
    "target['Value']=df['Value']-df[\"Value\"].min()\n",
    "input_tonumpy=input.to_numpy()\n",
    "input_tensor = torch.from_numpy(input_tonumpy).to(torch.float32)\n",
    "target_tonumpy=target.to_numpy()\n",
    "target_tensor = torch.from_numpy(target_tonumpy).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2994b63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1986.],\n",
       "        [1984.],\n",
       "        [1934.],\n",
       "        ...,\n",
       "        [1559.],\n",
       "        [1560.],\n",
       "        [1559.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e8a48d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  0.],\n",
       "        [ 1.,  1.,  0.],\n",
       "        [ 1.,  1.,  0.],\n",
       "        ...,\n",
       "        [ 5.,  1., 23.],\n",
       "        [ 5.,  1., 23.],\n",
       "        [ 5.,  1., 23.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883acd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1984.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c25bf854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "smoothing_constant = 0.5\n",
    "\n",
    "# Define the recurrent neural network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.prev_input=1\n",
    "        \n",
    "    def forward(self, input):\n",
    "    # Use exponential smoothing on the input\n",
    "        smoothed_input = input * smoothing_constant + (1 - smoothing_constant) * self.prev_input\n",
    "        self.prev_input = smoothed_input\n",
    "\n",
    "    # Pass the smoothed input through the LSTM layer\n",
    "        lstm_out, hidden = self.lstm(smoothed_input.view(len(input), 1, -1))\n",
    "\n",
    "    # Pass the LSTM output through the fully-connected layer\n",
    "        output = self.fc(lstm_out.view(len(input), -1))\n",
    "        return output\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNN(1, 128, 1)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = SMAPELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a171ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 8.4014\n",
      "Epoch 2: Loss = 8.4148\n",
      "Epoch 3: Loss = 8.1446\n",
      "Epoch 4: Loss = 8.0112\n",
      "Epoch 5: Loss = 8.1470\n",
      "Epoch 6: Loss = 8.4425\n",
      "Epoch 7: Loss = 7.9066\n",
      "Epoch 8: Loss = 7.1465\n",
      "Epoch 9: Loss = 7.1472\n",
      "Epoch 10: Loss = 7.3415\n",
      "Epoch 11: Loss = 7.2435\n",
      "Epoch 12: Loss = 7.0839\n",
      "Epoch 13: Loss = 7.4909\n",
      "Epoch 14: Loss = 7.8744\n",
      "Epoch 15: Loss = 7.3675\n",
      "Epoch 16: Loss = 7.3687\n",
      "Epoch 17: Loss = 7.4036\n",
      "Epoch 18: Loss = 7.3290\n",
      "Epoch 19: Loss = 7.1996\n",
      "Epoch 20: Loss = 6.8254\n",
      "Epoch 21: Loss = 6.7529\n",
      "Epoch 22: Loss = 6.4671\n",
      "Epoch 23: Loss = 7.0538\n",
      "Epoch 24: Loss = 6.7941\n",
      "Epoch 25: Loss = 6.9835\n",
      "Epoch 26: Loss = 6.4110\n",
      "Epoch 27: Loss = 5.9486\n",
      "Epoch 28: Loss = 6.6391\n",
      "Epoch 29: Loss = 6.1255\n",
      "Epoch 30: Loss = 6.1857\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs=30\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_tensor)):\n",
    "        # Make predictions using the model\n",
    "        output = model.forward(input_tensor[i])\n",
    "    \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target_tensor[i])\n",
    "        #loss = smape.loss(output, target_tensor[i])\n",
    "        # Update the model's parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}')\n",
    "#     print(f'Epoch {epoch+1}: SMAPE = {loss.item():.4f}')\n",
    "#         print(input_tensor[i].type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f6fb19b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1059.0864],\n",
       "        [1081.5814],\n",
       "        [1085.0291]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
